{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Modelling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ruman-shaikh/NLP_Project_Grp_1/blob/Jasleen/Modelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "6KtAjOKSmwdA",
        "outputId": "44805b55-57ec-4919-afaa-d6b352dfd84a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "y5u3FBcXnWUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "KvV9TxN8ihNY"
      },
      "outputs": [],
      "source": [
        "## This is the RNN provided in the NMA Sentiment Analysis Template. I \n",
        "\n",
        "class SentimentRNN(nn.Module):\n",
        "  def __init__(self,no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.1):\n",
        "    super(SentimentRNN,self).__init__()\n",
        "\n",
        "    self.output_dim = output_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.no_layers = no_layers\n",
        "    self.vocab_size = vocab_size\n",
        "    self.drop_prob = drop_prob\n",
        "\n",
        "    # Embedding Layer\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    # LSTM Layers\n",
        "    self.lstm = nn.LSTM(input_size=embedding_dim,hidden_size=self.hidden_dim,\n",
        "                        num_layers=no_layers, batch_first=True,\n",
        "                        dropout=self.drop_prob)\n",
        "\n",
        "    # Dropout layer\n",
        "    self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "    # Linear and Sigmoid layer\n",
        "    self.fc = nn.Linear(self.hidden_dim, output_dim)\n",
        "    self.sig = nn.Sigmoid()\n",
        "\n",
        "  def forward(self,x,hidden):\n",
        "    batch_size = x.size(0)\n",
        "\n",
        "    # Embedding out\n",
        "    embeds = self.embedding(x)\n",
        "    #Shape: [batch_size x max_length x embedding_dim]\n",
        "\n",
        "    # LSTM out\n",
        "    lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "    # Shape: [batch_size x max_length x hidden_dim]\n",
        "\n",
        "    # Select the activation of the last Hidden Layer\n",
        "    lstm_out = lstm_out[:,-1,:].contiguous()\n",
        "    # Shape: [batch_size x hidden_dim]\n",
        "\n",
        "    ## You can instead average the activations across all the times\n",
        "    # lstm_out = torch.mean(lstm_out, 1).contiguous()\n",
        "\n",
        "    # Dropout and Fully connected layer\n",
        "    out = self.dropout(lstm_out)\n",
        "    out = self.fc(out)\n",
        "\n",
        "    # Sigmoid function\n",
        "    sig_out = self.sig(out)\n",
        "\n",
        "    # return last sigmoid output and hidden state\n",
        "    return sig_out, hidden\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    ''' Initializes hidden state '''\n",
        "    # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "    # initialized to zero, for hidden state and cell state of LSTM\n",
        "    h0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
        "    c0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
        "    hidden = (h0,c0)\n",
        "    return hidden"
      ]
    }
  ]
}